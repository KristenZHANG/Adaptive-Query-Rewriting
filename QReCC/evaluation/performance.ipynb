{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytrec_eva'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytrec_eva\u001b[39;00m\n\u001b[1;32m      6\u001b[0m l\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytrec_eva'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pytrec_eva\n",
    "l\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision_k(prediction, qrels):\n",
    "    r_1_p = r_5_p = r_10_p = r_50_p = r_100_p = total = 0\n",
    "    for item in prediction.keys():\n",
    "        assert list(qrels[item].keys())[0] != ''\n",
    "\n",
    "        hypo = [k for k, v in sorted(prediction[item].items(), key=lambda item: item[1], reverse=True)]\n",
    "        ref = set(list(qrels[item].keys()))\n",
    "\n",
    "        total += 1\n",
    "        \n",
    "        r_1_p += len(set(hypo[:1]) & ref) / len(ref)\n",
    "        r_5_p += len(set(hypo[:5]) & ref) / len(ref)\n",
    "        r_10_p += len(set(hypo[:10]) & ref) / len(ref)\n",
    "        r_50_p += len(set(hypo[:50]) & ref) / len(ref)\n",
    "        r_100_p += len(set(hypo[:100]) & ref) / len(ref)\n",
    "\n",
    "    r_1_p = 100.0 * r_1_p / total\n",
    "    r_5_p = 100.0 * r_5_p / total\n",
    "    r_10_p = 100.0 * r_10_p / total\n",
    "    r_50_p = 100.0 * r_50_p / total\n",
    "    r_100_p = 100.0 * r_100_p / total\n",
    "\n",
    "    print(f\"Pid_Prec@1: {r_1_p: .2f}\")\n",
    "    # print(f\"Pid_Prec@5: {r_5_p: .2f}\")\n",
    "    # print(f\"Pid_Prec@10: {r_10_p: .2f}\")\n",
    "    print(f\"Pid_Prec@50: {r_50_p: .2f}\")\n",
    "    # print(f\"Pid_Prec@100: {r_100_p: .2f}\")\n",
    "\n",
    "\n",
    "def offical_eval(all_result, all_qrels):\n",
    "    pids = list(all_result.keys())\n",
    "\n",
    "    sqrels = dict(filter(lambda x: x[0] in pids, all_qrels.items()))\n",
    "    sqrels = dict(filter(lambda x: x[1] != {\"\": 1}, sqrels.items()))\n",
    "    sresults = dict(filter(lambda x: x[0] in pids, all_result.items()))\n",
    "    # print(len(sqrels), len(sresults))\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(\n",
    "                    sqrels, {\"recip_rank\", \"recall\", \"map\", \"ndcg\", \"ndcg_cut_3\"})\n",
    "    metrics = evaluator.evaluate(sresults)\n",
    "    mrr_list = [v[\"recip_rank\"] for v in metrics.values()]\n",
    "    recall_5_list = [v[\"recall_5\"] for v in metrics.values()]\n",
    "    recall_10_list = [v[\"recall_10\"] for v in metrics.values()]\n",
    "    recall_20_list = [v[\"recall_20\"] for v in metrics.values()]\n",
    "    recall_30_list = [v[\"recall_30\"] for v in metrics.values()]\n",
    "    map_list = [v[\"map\"] for v in metrics.values()]\n",
    "    ndcg_list = [v[\"ndcg\"] for v in metrics.values()]\n",
    "    ndcg_3_list = [v[\"ndcg_cut_3\"] for v in metrics.values()]\n",
    "\n",
    "\n",
    "    eval_metrics = {\n",
    "        \"MRR\": round(100*np.average(mrr_list), 2),\n",
    "        \"map\": round(100*np.average(map_list), 2),\n",
    "        \"ndcg@3\": round(100*np.average(ndcg_3_list), 2),\n",
    "        \"Recall@5\": round(100*np.average(recall_5_list), 2),\n",
    "        \"Recall@10\": round(100*np.average(recall_10_list), 2),\n",
    "    }\n",
    "    print(eval_metrics)\n",
    "    \n",
    "def compute_mrr_k(prediction, qrels):\n",
    "    mrr_depths = [1,5,10]\n",
    "    mrr_sums = {depth: 0.0 for depth in mrr_depths}\n",
    "    for item in prediction.keys():\n",
    "        assert list(qrels[item].keys())[0] != ''\n",
    "        gold_pids = list(qrels[item].keys())\n",
    "        hypo = [k for k, v in sorted(prediction[item].items(), key=lambda item: item[1], reverse=True)]\n",
    "        positive_ranks = [i for i, pid in enumerate(hypo) if pid in gold_pids]\n",
    "        if len(positive_ranks) == 0: # no gt pid in results / original retriever result\n",
    "            continue\n",
    "        first_positive = positive_ranks[0]\n",
    "        for depth in mrr_depths:\n",
    "            mrr_sums[depth] += (1.0 / (first_positive+1.0)) if first_positive < depth else 0.0\n",
    "    for depth in mrr_depths:\n",
    "        mrr_sums[depth] = mrr_sums[depth] / len(prediction) * 100\n",
    "    print(mrr_sums)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16451, 1357)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "qrels = json.load(open(\"{path}/qrels_test.txt\"))\n",
    "\n",
    "# since our work do not deal with first turns, we can just use the provided rewrite by the project (Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting) and get corresponding retrieval results. The first-turn results are the same for all different approaches.\n",
    "firstturn_rewrite = json.load(open('{path_to_firstturn_rewrite_result}'))\n",
    "\n",
    "len(qrels), len(firstturn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_all_gpt_sft_mistral_bm25.json 6852 16451\n",
      " *** rewrite 8209 ***\n",
      "Pid_Prec@1:  32.39\n",
      "Pid_Prec@50:  83.67\n",
      "{'MRR': 45.85, 'map': 44.39, 'ndcg@3': 43.15, 'Recall@5': 56.0, 'Recall@10': 64.92}\n",
      "test_all_dpo_with_qa_loss_bm25_temp1.top50_bm25.json 6852 16451\n",
      " *** rewrite 8209 ***\n",
      "Pid_Prec@1:  39.75\n",
      "Pid_Prec@50:  84.99\n",
      "{'MRR': 52.33, 'map': 50.76, 'ndcg@3': 49.9, 'Recall@5': 61.32, 'Recall@10': 69.06}\n",
      "test_all_dpo_with_gold_reward.top50_bm25.json 6852 16451\n",
      " *** rewrite 8209 ***\n",
      "Pid_Prec@1:  38.82\n",
      "Pid_Prec@50:  86.76\n",
      "{'MRR': 51.89, 'map': 50.33, 'ndcg@3': 49.4, 'Recall@5': 61.55, 'Recall@10': 69.79}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = f'./result/'\n",
    "\n",
    "for file in sorted(glob.glob(f\"{file_path}/*.json\"), reverse=True):\n",
    "    prediction = json.load(open(f'{file}'))\n",
    "    print(file.split('/')[-1], len(prediction.keys()), len(qrels))\n",
    "    # print(f'******non-first-turns: {len(prediction)}******')\n",
    "    # compute_precision_k(prediction, qrels)\n",
    "    # offical_eval(prediction,qrels)\n",
    "\n",
    "\n",
    "    all_prediction_rewrite = prediction.copy()\n",
    "    all_prediction_rewrite.update(firstturn_rewrite)\n",
    "\n",
    "\n",
    "    print(f' *** rewrite {len(all_prediction_rewrite)} ***')\n",
    "    compute_precision_k(all_prediction_rewrite, qrels)\n",
    "    offical_eval(all_prediction_rewrite,qrels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conv-rewrite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
